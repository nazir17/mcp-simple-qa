MCP Server for API Q&A from Postman Collections

This project is a pure MCP (Model Context Protocol) server that allows users to ask natural-language questions about APIs using Postman collection files as the knowledge source.

It is designed to be tested and used via MCP-compatible clients, such as MCP Inspector, copilot and claude desktop.

âœ¨ Key Features

ğŸ“„ Load Postman collection JSON files

ğŸ” Index API endpoints using vector embeddings

ğŸ¤– Answer questions exclusively via Google Gemini LLM

ğŸ§© MCP-native design using mcp-use

ğŸ§ª Tested via MCP Inspector and copilot


ğŸ—ï¸ Architecture Overview
Postman Collection (JSON)
        â†“
MCP Tool: load_postman_collection
        â†“
Vector Index (SentenceTransformers + FAISS)
        â†“
MCP Tool: ask_llm
        â†“
Google Gemini LLM
        â†“
Answer (grounded in API context)

ğŸ“‚ Project Structure
mcp-simple-qa/
â”‚
â”œâ”€â”€ server.py              # MCP server entry point
â”œâ”€â”€ google_llm.py          # Google Gemini LLM wrapper
â”œâ”€â”€ postman_loader.py      # Postman collection parser
â”œâ”€â”€ qa_index.py            # Embeddings + FAISS index
â”œâ”€â”€ requirements.txt       # Project dependencies
â””â”€â”€ README.md

âš™ï¸ Requirements

Python 3.9+

Linux / macOS / Windows

Google Generative AI API key

ğŸ” Environment Setup

Create a .env file in the project root:

GOOGLE_API_KEY=your_google_api_key_here

ğŸ Python Setup (Recommended)

Use a virtual environment:

python -m venv venv
source venv/bin/activate


Install dependencies:

pip install -r requirements.txt

ğŸš€ Running the MCP Server

This project is not run like a normal web server.

Start it using the MCP CLI:

mcp dev server.py


This will:

Launch the MCP server

Start the MCP Inspector

Open Inspector in your browser automatically

ğŸ§ª Testing with MCP Inspector
1ï¸âƒ£ Load a Postman Collection

Use the tool:

load_postman_collection_tool

Input:

Paste the entire Postman collection JSON as a string

Expected output:

{
  "status": "ok",
  "indexed_endpoints": 5
}

2ï¸âƒ£ Ask Questions About the API

Use the tool:

ask_llm

Example input:

{
  "question": "What HTTP method is used to create a user?"
}


Example output:

{
  "question": "What HTTP method is used to create a user?",
  "answer": "The API uses the POST method to create a user."
}


All answers are generated only by the LLM, grounded in the indexed API documentation.

ğŸ“¦ Dependencies

Key libraries used:

mcp-use â€“ MCP server framework

sentence-transformers â€“ Embeddings

faiss-cpu â€“ Vector similarity search

google-generativeai â€“ Google Gemini LLM

python-dotenv â€“ Environment variable loading

numpy â€“ Numerical operations

ğŸ§  Design Philosophy

This project follows MCP-native principles:

Tools over endpoints

Context over raw retrieval

LLM-only answers (no extractive APIs)

Clean separation of concerns

Minimal, inspectable, and debuggable design

ğŸ“œ License

MIT License

ğŸ™Œ Acknowledgements

Model Context Protocol (MCP)

Google Generative AI

SentenceTransformers

FAISS